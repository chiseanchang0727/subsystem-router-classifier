# Sampling-Based Human Validation

The training data in this project is **AI-generated and weakly labeled**, meaning subsystem labels are not guaranteed to be perfectly correct. At the early stage of development, **no real, human-labeled dataset exists** that can be used as a reference for validating synthetic data quality.

To address this, we adopt a **bootstrap validation process** that progressively converts synthetic data into a reliable reference through **sampling-based human validation**, and then uses this reference to evaluate the remaining synthetic data.

The validation process follows three explicit steps:

1. **Generate synthetic data**
2. **Sample and human-validate a subset**
3. **Compare distributions between validated data and remaining synthetic data**

---

## Validation Workflow

### Step 1 — Synthetic Data Generation

All training samples are initially generated by AI and treated as **weak labels**. These labels are assumed to be *noisy but useful*, and are not considered ground truth until validated.

---

### Step 2 — Sampling-Based Human Validation

Because human validation is costly, manually reviewing every synthetic sample is impractical. Instead, we apply a **statistically grounded sampling strategy** to select a subset of synthetic data for expert review.

At this stage, **no model confidence scores are available**, since the classifier has not yet been trained. Therefore, sampling is performed using **observable properties of the data itself**, rather than prediction confidence.

#### Sampling Strategy

Samples are stratified using factors known to correlate with labeling difficulty:

- **Number of active subsystems**  
  (single-subsystem vs. multi-subsystem queries)
- **Presence of regulation-related or image-related subsystems**
- **Query complexity**, estimated using simple heuristics such as:
  - query length
  - conjunctions (e.g., “and”, “or”, “also”)
  - multiple question markers

Multi-subsystem and complex queries are intentionally **oversampled**, as these cases are more ambiguous and more likely to contain labeling errors.

#### Sample Size

For a dataset of approximately **1,000 synthetic samples**, auditing around **280 samples** is sufficient to estimate overall label correctness with **95% confidence** and a **±5% margin of error**.

When oversampling higher-risk strata, **weighted evaluation** is applied to ensure that overall estimates remain statistically unbiased.

#### Weighted Evaluation

When certain strata are oversampled (e.g., multi-subsystem queries), each validated sample is assigned a weight based on its stratum's representation in the full dataset. This prevents oversampled groups from dominating quality estimates.

For example, if multi-subsystem queries represent 30% of the full dataset but 50% of the validated subset, each multi-subsystem sample receives a weight of 0.6 (30% / 50%). The overall correctness rate is then calculated as:

$$\text{Overall Correctness} = \frac{\sum_{i} w_i \cdot \text{correct}_i}{\sum_{i} w_i}$$

where $w_i$ is the weight for sample $i$ and $\text{correct}_i$ is 1 if the label is correct, 0 otherwise.

#### Validation Process and Quality Control

**Inter-annotator Agreement**: For ambiguous or complex cases, multiple validators review the same samples. We measure inter-annotator agreement using Cohen's kappa for categorical labels. Samples with low agreement (κ < 0.6) are flagged for discussion and consensus.

**Handling Disagreements**: 
- If validators disagree on a label, the sample is reviewed by a senior domain expert
- Edge cases are documented and used to refine labeling guidelines
- Ambiguous queries may be excluded from training or assigned probabilistic labels

**Validation Guidelines**: Validators follow explicit criteria:
- Review the query in context of subsystem definitions
- Check for multi-label correctness (all relevant subsystems should be marked)
- Verify causal vs. pure regulation distinction based on stage context
- Confirm image request detection based on explicit visual indicators

---

### Step 3 — Distribution Comparison Using Validated Subset

Once sampled synthetic data has been reviewed and corrected by domain experts, this subset is treated as a **human-validated reference dataset**.

Rather than comparing synthetic data against an external real dataset (which does not yet exist), we compare the **distribution of the remaining synthetic data** against the **distribution of the validated subset**.

Examples of distributional signals include:

- Subsystem label frequencies
- Co-occurrence patterns between subsystems
- Query complexity distributions
- Single vs. multi-subsystem ratios

Significant distribution drift between the validated subset and the remaining synthetic data indicates **systematic labeling bias or generation errors**, and provides actionable feedback for improving synthetic data generation.

#### Distribution Comparison Metrics

We use multiple statistical tests to detect distribution drift:

1. **Chi-square test** for subsystem label frequencies
   - Compares observed frequencies in validated vs. remaining synthetic data
   - Significance threshold: p < 0.05 indicates significant drift

2. **Kolmogorov-Smirnov test** for query complexity distributions
   - Tests if complexity scores follow the same distribution
   - Significance threshold: p < 0.05

3. **Jaccard similarity** for co-occurrence patterns
   - Measures similarity of subsystem co-occurrence matrices
   - Threshold: Jaccard < 0.85 indicates notable drift

4. **Proportion difference tests** for single vs. multi-subsystem ratios
   - Z-test for proportions with Bonferroni correction
   - Significance threshold: p < 0.01 (adjusted for multiple comparisons)

**Significant drift** is defined as:
- At least 2 of the above tests showing significant differences (p < threshold), OR
- A single test showing very strong drift (p < 0.001) with practical impact (>10% absolute difference in key metrics)

#### Actionable Outcomes

When significant drift is detected:

1. **Root Cause Analysis**
   - Identify which subsystems or query types show the largest discrepancies
   - Analyze patterns in mislabeled samples (e.g., systematic confusion between causal and pure regulation)

2. **Remediation Actions**
   - **Prompt refinement**: Update the generation prompt to address identified biases
   - **Selective regeneration**: Regenerate only the problematic strata (e.g., all multi-subsystem queries)
   - **Label correction**: If drift is localized, manually correct remaining synthetic labels in affected categories
   - **Data augmentation**: Generate additional samples for underrepresented or problematic categories

3. **Iterative Improvement**
   - Re-run validation on regenerated/corrected data
   - Compare new distributions against the validated reference
   - Continue until drift is within acceptable thresholds

4. **Documentation**
   - Record identified biases and their causes
   - Update generation guidelines to prevent recurrence
   - Track validation metrics across iterations

---

## Sample Size Calculation

The initial sample size is calculated using the standard formula for estimating a population proportion:

$$
n = \frac{z^2 \cdot p(1 - p)}{e^2}
$$

where:
- $z = 1.96$ (95% confidence)
- $p$ is the expected correctness rate (set to 0.5 as a conservative worst-case)
- $e = 0.05$ is the desired margin of error

This yields an initial estimate of approximately **385 samples**.

Because the dataset size is finite, we apply **finite population correction**:

$$
n_{\text{adjusted}} = \frac{n}{1 + \frac{n - 1}{N}}
$$

where $N = 1000$, resulting in a final sample size of approximately **280 samples**.

---

## Benefits

This staged validation approach allows us to:

- Bootstrap a reliable reference dataset from synthetic data
- Quantify overall label quality with statistical guarantees
- Detect systematic synthetic labeling bias early
- Enable distribution-based validation **without requiring real data upfront**
- Improve synthetic data generation before model training

Once an initial classifier is trained, **model confidence scores can be incorporated into future validation rounds**, further focusing human effort on uncertain or high-risk samples.
